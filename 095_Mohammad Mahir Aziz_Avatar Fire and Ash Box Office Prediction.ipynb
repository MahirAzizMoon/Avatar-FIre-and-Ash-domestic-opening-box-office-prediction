{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Avatar: Fire and Ash domestic opening week box office prediction**"
      ],
      "metadata": {
        "id": "4lwfpLFlExKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import warnings\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "yBO_DorZymsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TMDB_API_KEY = \"\"\n",
        "TOTAL_MOVIES = 10000"
      ],
      "metadata": {
        "id": "zeL-FwHfyy92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TMDB REQUEST\n"
      ],
      "metadata": {
        "id": "PirNAHdSnlib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tmdb_request(url, params):\n",
        "    \"\"\"Make a request to TMDB API\"\"\"\n",
        "    params['api_key'] = TMDB_API_KEY\n",
        "    response = requests.get(url, params=params)\n",
        "    response.raise_for_status()\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "AroyID4TnROX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FETCH MOVIES\n"
      ],
      "metadata": {
        "id": "Q-ZX4o97npEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_popular_movies(total_movies):\n",
        "    \"\"\"Get list of popular movies from TMDB\"\"\"\n",
        "    print(f\"[TMDB] Fetching {total_movies} popular movies...\")\n",
        "\n",
        "    movies = []\n",
        "    page = 1\n",
        "    max_pages = min(500, (total_movies // 20) + 1)\n",
        "\n",
        "    while page <= max_pages and len(movies) < total_movies:\n",
        "        url = \"https://api.themoviedb.org/3/discover/movie\"\n",
        "        params = {\n",
        "            \"sort_by\": \"popularity.desc\",\n",
        "            \"page\": page,\n",
        "            \"include_adult\": \"false\",\n",
        "            \"language\": \"en-US\",\n",
        "            \"vote_count.gte\": \"100\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            data = make_tmdb_request(url, params)\n",
        "            movies.extend(data['results'])\n",
        "            print_progress_bar(len(movies), total_movies, \"Collecting movies\")\n",
        "            page += 1\n",
        "            time.sleep(0.25)\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n[TMDB] ✓ Collected {len(movies)} movies\\n\")\n",
        "    return movies[:total_movies]"
      ],
      "metadata": {
        "id": "nXNJ4qGjnSeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RETRIEVE MOVIE DETAILS\n"
      ],
      "metadata": {
        "id": "1PpXm7Cznybe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_details(movie_id):\n",
        "    \"\"\"Get detailed information for a single movie\"\"\"\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}\"\n",
        "    params = {\"append_to_response\": \"credits,release_dates\"}\n",
        "\n",
        "    try:\n",
        "        return make_tmdb_request(url, params)\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ErhDt2MHnVwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPAA RATING\n",
        "\n",
        "This function extracts the US MPAA rating (such as PG or R) from the movie’s release data.\n"
      ],
      "metadata": {
        "id": "uYeEf0Rkn2G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mpaa_rating(release_dates):\n",
        "    \"\"\"Extract MPAA rating (PG, PG-13, R, etc.) from release dates\"\"\"\n",
        "    mpaa_rating = ''\n",
        "    results = release_dates.get('results', [])\n",
        "\n",
        "    for country_data in results:\n",
        "        if country_data['iso_3166_1'] == 'US':\n",
        "            for release in country_data['release_dates']:\n",
        "                cert = release.get('certification', '')\n",
        "                if cert:\n",
        "                    mpaa_rating = cert\n",
        "                    break\n",
        "            if mpaa_rating:\n",
        "                break\n",
        "\n",
        "    return mpaa_rating\n"
      ],
      "metadata": {
        "id": "RSjXejwznX0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURE EXTRACTION\n"
      ],
      "metadata": {
        "id": "vLwN0QUqn5zx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoVfmkAYv-Q0"
      },
      "outputs": [],
      "source": [
        "def extract_movie_data(details):\n",
        "    \"\"\"Extract all 17 features from movie details\"\"\"\n",
        "\n",
        "    # Feature 1-5: Basic info\n",
        "    title = details.get('title', '')\n",
        "    release_date = details.get('release_date', '')\n",
        "    tmdb_id = details.get('id', '')\n",
        "    imdb_id = details.get('imdb_id', '')\n",
        "    budget = details.get('budget', 0)\n",
        "\n",
        "    # Feature 6: Revenue from TMDB\n",
        "    revenue_tmdb = details.get('revenue', 0)\n",
        "\n",
        "    # Feature 7-8: Genres and runtime\n",
        "    genres = '|'.join([g['name'] for g in details.get('genres', [])])\n",
        "    runtime = details.get('runtime', '')\n",
        "\n",
        "    # Feature 9: Country\n",
        "    countries = details.get('production_countries', [])\n",
        "    country = countries[0]['iso_3166_1'] if countries else ''\n",
        "\n",
        "    # Feature 10: MPAA rating\n",
        "    mpaa_rating = extract_mpaa_rating(details.get('release_dates', {}))\n",
        "\n",
        "    # Feature 11-12: Popularity and franchise\n",
        "    popularity = round(details.get('popularity', 0), 2)\n",
        "    is_franchise = 1 if details.get('belongs_to_collection') else 0\n",
        "\n",
        "    # Feature 13: Director\n",
        "    crew = details.get('credits', {}).get('crew', [])\n",
        "    directors = [c['name'] for c in crew if c['job'] == 'Director']\n",
        "    director = directors[0] if directors else ''\n",
        "\n",
        "    # Feature 14: Actors (top 5)\n",
        "    cast = details.get('credits', {}).get('cast', [])\n",
        "    actors = '|'.join([a['name'] for a in cast[:5]])\n",
        "\n",
        "    # Feature 15: Vote average\n",
        "    vote_average = details.get('vote_average', '')\n",
        "\n",
        "    # Feature 16: Production companies\n",
        "    prod_companies = details.get('production_companies', [])\n",
        "    production_companies = '|'.join([pc['name'] for pc in prod_companies])\n",
        "\n",
        "    # Feature 17: Distributor from TMDB\n",
        "    distributor_tmdb = prod_companies[0]['name'] if prod_companies else ''\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'release_date': release_date,\n",
        "        'tmdb_id': tmdb_id,\n",
        "        'imdb_id': imdb_id,\n",
        "        'budget': budget,\n",
        "        'revenue_tmdb': revenue_tmdb,\n",
        "        'genres': genres,\n",
        "        'runtime': runtime,\n",
        "        'country': country,\n",
        "        'mpaa_rating': mpaa_rating,\n",
        "        'popularity': popularity,\n",
        "        'is_franchise': is_franchise,\n",
        "        'director': director,\n",
        "        'actors': actors,\n",
        "        'vote_average': vote_average,\n",
        "        'production_companies': production_companies,\n",
        "        'distributor_tmdb': distributor_tmdb\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TMDB SCRAPING\n"
      ],
      "metadata": {
        "id": "X-fVoZdvn9R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_tmdb_movies(total_movies):\n",
        "\n",
        "    # Step 1: Get list of popular movies\n",
        "    movie_list = get_popular_movies(total_movies)\n",
        "\n",
        "    # Step 2: Get detailed data for each movie\n",
        "    all_movies = []\n",
        "    print(\"[TMDB] Fetching detailed data for each movie...\")\n",
        "\n",
        "    for i, movie_basic in enumerate(movie_list, 1):\n",
        "        movie_id = movie_basic['id']\n",
        "        details = get_movie_details(movie_id)\n",
        "\n",
        "        if details:\n",
        "            movie_data = extract_movie_data(details)\n",
        "            all_movies.append(movie_data)\n",
        "\n",
        "        # Show progress bar\n",
        "        print_progress_bar(i, len(movie_list), \"Scraping details\")\n",
        "\n",
        "        # Show milestone messages\n",
        "        if i % 100 == 0:\n",
        "            print(f\"\\n  ✓ {i} movies completed!\")\n",
        "            print_progress_bar(i, len(movie_list), \"Scraping details\")\n",
        "\n",
        "        time.sleep(0.25)  # Be nice to the API\n",
        "\n",
        "    print(f\"\\n\\n[TMDB] ✓ Scraped {len(all_movies)} movies with 17 features\\n\")\n",
        "    return all_movies\n"
      ],
      "metadata": {
        "id": "86xbPgoOnhA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROGRESS BAR\n"
      ],
      "metadata": {
        "id": "E0AnTSPIn_9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_progress_bar(current, total, prefix=\"Progress\"):\n",
        "    \"\"\"Print a nice progress bar\"\"\"\n",
        "    percentage = (current / total) * 100\n",
        "    bar_length = 40\n",
        "    filled = int(bar_length * current / total)\n",
        "    bar = '█' * filled + '░' * (bar_length - filled)\n",
        "    print(f\"\\r  {prefix}: [{bar}] {percentage:.1f}% ({current}/{total})\", end='', flush=True)"
      ],
      "metadata": {
        "id": "4W6t0CmnneoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA COLLECTION\n",
        "\n",
        "This section runs the movie scraper, converts the collected data into a pandas DataFrame, and saves it as a CSV file."
      ],
      "metadata": {
        "id": "z4sNeJJAoIYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TMDB MOVIE SCRAPER\")\n",
        "\n",
        "# Scrape movies\n",
        "movies_data = scrape_tmdb_movies(TOTAL_MOVIES)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(movies_data)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = 'tmdb_data_17_features.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "\n",
        "print(\"✓ SCRAPING COMPLETE!\")\n",
        "print(f\"\\nOutput file: {output_file}\")\n",
        "print(f\"Total movies: {len(df)}\")\n",
        "print(f\"Total features: {len(df.columns)}\")\n",
        "\n",
        "print(f\"\\n17 Features collected:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "\n",
        "print(f\"\\n✓ Data saved to '{output_file}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT9om7ePy9md",
        "outputId": "042dc719-60c5-47b0-8c86-27a30e68f8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TMDB MOVIE SCRAPER\n",
            "[TMDB] Fetching 10000 popular movies...\n",
            "  Collecting movies: [████████████████████████████████████████] 100.0% (10000/10000)\n",
            "[TMDB] ✓ Collected 10000 movies\n",
            "\n",
            "[TMDB] Fetching detailed data for each movie...\n",
            "  Scraping details: [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.0% (100/10000)\n",
            "  ✓ 100 movies completed!\n",
            "  Scraping details: [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.0% (200/10000)\n",
            "  ✓ 200 movies completed!\n",
            "  Scraping details: [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 3.0% (300/10000)\n",
            "  ✓ 300 movies completed!\n",
            "  Scraping details: [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.0% (400/10000)\n",
            "  ✓ 400 movies completed!\n",
            "  Scraping details: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5.0% (500/10000)\n",
            "  ✓ 500 movies completed!\n",
            "  Scraping details: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 6.0% (600/10000)\n",
            "  ✓ 600 movies completed!\n",
            "  Scraping details: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.0% (700/10000)\n",
            "  ✓ 700 movies completed!\n",
            "  Scraping details: [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8.0% (800/10000)\n",
            "  ✓ 800 movies completed!\n",
            "  Scraping details: [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.0% (900/10000)\n",
            "  ✓ 900 movies completed!\n",
            "  Scraping details: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10.0% (1000/10000)\n",
            "  ✓ 1000 movies completed!\n",
            "  Scraping details: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.0% (1100/10000)\n",
            "  ✓ 1100 movies completed!\n",
            "  Scraping details: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12.0% (1200/10000)\n",
            "  ✓ 1200 movies completed!\n",
            "  Scraping details: [█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 13.0% (1300/10000)\n",
            "  ✓ 1300 movies completed!\n",
            "  Scraping details: [█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.0% (1400/10000)\n",
            "  ✓ 1400 movies completed!\n",
            "  Scraping details: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.0% (1500/10000)\n",
            "  ✓ 1500 movies completed!\n",
            "  Scraping details: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.0% (1600/10000)\n",
            "  ✓ 1600 movies completed!\n",
            "  Scraping details: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 17.0% (1700/10000)\n",
            "  ✓ 1700 movies completed!\n",
            "  Scraping details: [███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18.0% (1800/10000)\n",
            "  ✓ 1800 movies completed!\n",
            "  Scraping details: [███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19.0% (1900/10000)\n",
            "  ✓ 1900 movies completed!\n",
            "  Scraping details: [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20.0% (2000/10000)\n",
            "  ✓ 2000 movies completed!\n",
            "  Scraping details: [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21.0% (2100/10000)\n",
            "  ✓ 2100 movies completed!\n",
            "  Scraping details: [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22.0% (2200/10000)\n",
            "  ✓ 2200 movies completed!\n",
            "  Scraping details: [█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23.0% (2300/10000)\n",
            "  ✓ 2300 movies completed!\n",
            "  Scraping details: [█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 24.0% (2400/10000)\n",
            "  ✓ 2400 movies completed!\n",
            "  Scraping details: [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.0% (2500/10000)\n",
            "  ✓ 2500 movies completed!\n",
            "  Scraping details: [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26.0% (2600/10000)\n",
            "  ✓ 2600 movies completed!\n",
            "  Scraping details: [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27.0% (2700/10000)\n",
            "  ✓ 2700 movies completed!\n",
            "  Scraping details: [███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28.0% (2800/10000)\n",
            "  ✓ 2800 movies completed!\n",
            "  Scraping details: [███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.0% (2900/10000)\n",
            "  ✓ 2900 movies completed!\n",
            "  Scraping details: [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.0% (3000/10000)\n",
            "  ✓ 3000 movies completed!\n",
            "  Scraping details: [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 31.0% (3100/10000)\n",
            "  ✓ 3100 movies completed!\n",
            "  Scraping details: [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32.0% (3200/10000)\n",
            "  ✓ 3200 movies completed!\n",
            "  Scraping details: [█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33.0% (3300/10000)\n",
            "  ✓ 3300 movies completed!\n",
            "  Scraping details: [█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34.0% (3400/10000)\n",
            "  ✓ 3400 movies completed!\n",
            "  Scraping details: [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 35.0% (3500/10000)\n",
            "  ✓ 3500 movies completed!\n",
            "  Scraping details: [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.0% (3600/10000)\n",
            "  ✓ 3600 movies completed!\n",
            "  Scraping details: [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 37.0% (3700/10000)\n",
            "  ✓ 3700 movies completed!\n",
            "  Scraping details: [███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 38.0% (3800/10000)\n",
            "  ✓ 3800 movies completed!\n",
            "  Scraping details: [███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39.0% (3900/10000)\n",
            "  ✓ 3900 movies completed!\n",
            "  Scraping details: [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40.0% (4000/10000)\n",
            "  ✓ 4000 movies completed!\n",
            "  Scraping details: [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41.0% (4100/10000)\n",
            "  ✓ 4100 movies completed!\n",
            "  Scraping details: [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 42.0% (4200/10000)\n",
            "  ✓ 4200 movies completed!\n",
            "  Scraping details: [█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.0% (4300/10000)\n",
            "  ✓ 4300 movies completed!\n",
            "  Scraping details: [█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44.0% (4400/10000)\n",
            "  ✓ 4400 movies completed!\n",
            "  Scraping details: [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 45.0% (4500/10000)\n",
            "  ✓ 4500 movies completed!\n",
            "  Scraping details: [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.0% (4600/10000)\n",
            "  ✓ 4600 movies completed!\n",
            "  Scraping details: [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 47.0% (4700/10000)\n",
            "  ✓ 4700 movies completed!\n",
            "  Scraping details: [███████████████████░░░░░░░░░░░░░░░░░░░░░] 48.0% (4800/10000)\n",
            "  ✓ 4800 movies completed!\n",
            "  Scraping details: [███████████████████░░░░░░░░░░░░░░░░░░░░░] 49.0% (4900/10000)\n",
            "  ✓ 4900 movies completed!\n",
            "  Scraping details: [████████████████████░░░░░░░░░░░░░░░░░░░░] 50.0% (5000/10000)\n",
            "  ✓ 5000 movies completed!\n",
            "  Scraping details: [████████████████████░░░░░░░░░░░░░░░░░░░░] 51.0% (5100/10000)\n",
            "  ✓ 5100 movies completed!\n",
            "  Scraping details: [████████████████████░░░░░░░░░░░░░░░░░░░░] 52.0% (5200/10000)\n",
            "  ✓ 5200 movies completed!\n",
            "  Scraping details: [█████████████████████░░░░░░░░░░░░░░░░░░░] 53.0% (5300/10000)\n",
            "  ✓ 5300 movies completed!\n",
            "  Scraping details: [█████████████████████░░░░░░░░░░░░░░░░░░░] 54.0% (5400/10000)\n",
            "  ✓ 5400 movies completed!\n",
            "  Scraping details: [██████████████████████░░░░░░░░░░░░░░░░░░] 55.0% (5500/10000)\n",
            "  ✓ 5500 movies completed!\n",
            "  Scraping details: [██████████████████████░░░░░░░░░░░░░░░░░░] 56.0% (5600/10000)\n",
            "  ✓ 5600 movies completed!\n",
            "  Scraping details: [██████████████████████░░░░░░░░░░░░░░░░░░] 57.0% (5700/10000)\n",
            "  ✓ 5700 movies completed!\n",
            "  Scraping details: [███████████████████████░░░░░░░░░░░░░░░░░] 58.0% (5800/10000)\n",
            "  ✓ 5800 movies completed!\n",
            "  Scraping details: [███████████████████████░░░░░░░░░░░░░░░░░] 59.0% (5900/10000)\n",
            "  ✓ 5900 movies completed!\n",
            "  Scraping details: [████████████████████████░░░░░░░░░░░░░░░░] 60.0% (6000/10000)\n",
            "  ✓ 6000 movies completed!\n",
            "  Scraping details: [████████████████████████░░░░░░░░░░░░░░░░] 61.0% (6100/10000)\n",
            "  ✓ 6100 movies completed!\n",
            "  Scraping details: [████████████████████████░░░░░░░░░░░░░░░░] 62.0% (6200/10000)\n",
            "  ✓ 6200 movies completed!\n",
            "  Scraping details: [█████████████████████████░░░░░░░░░░░░░░░] 63.0% (6300/10000)\n",
            "  ✓ 6300 movies completed!\n",
            "  Scraping details: [█████████████████████████░░░░░░░░░░░░░░░] 64.0% (6400/10000)\n",
            "  ✓ 6400 movies completed!\n",
            "  Scraping details: [██████████████████████████░░░░░░░░░░░░░░] 65.0% (6500/10000)\n",
            "  ✓ 6500 movies completed!\n",
            "  Scraping details: [██████████████████████████░░░░░░░░░░░░░░] 66.0% (6600/10000)\n",
            "  ✓ 6600 movies completed!\n",
            "  Scraping details: [██████████████████████████░░░░░░░░░░░░░░] 67.0% (6700/10000)\n",
            "  ✓ 6700 movies completed!\n",
            "  Scraping details: [███████████████████████████░░░░░░░░░░░░░] 68.0% (6800/10000)\n",
            "  ✓ 6800 movies completed!\n",
            "  Scraping details: [███████████████████████████░░░░░░░░░░░░░] 69.0% (6900/10000)\n",
            "  ✓ 6900 movies completed!\n",
            "  Scraping details: [████████████████████████████░░░░░░░░░░░░] 70.0% (7000/10000)\n",
            "  ✓ 7000 movies completed!\n",
            "  Scraping details: [████████████████████████████░░░░░░░░░░░░] 71.0% (7100/10000)\n",
            "  ✓ 7100 movies completed!\n",
            "  Scraping details: [████████████████████████████░░░░░░░░░░░░] 72.0% (7200/10000)\n",
            "  ✓ 7200 movies completed!\n",
            "  Scraping details: [█████████████████████████████░░░░░░░░░░░] 73.0% (7300/10000)\n",
            "  ✓ 7300 movies completed!\n",
            "  Scraping details: [█████████████████████████████░░░░░░░░░░░] 74.0% (7400/10000)\n",
            "  ✓ 7400 movies completed!\n",
            "  Scraping details: [██████████████████████████████░░░░░░░░░░] 75.0% (7500/10000)\n",
            "  ✓ 7500 movies completed!\n",
            "  Scraping details: [██████████████████████████████░░░░░░░░░░] 76.0% (7600/10000)\n",
            "  ✓ 7600 movies completed!\n",
            "  Scraping details: [██████████████████████████████░░░░░░░░░░] 77.0% (7700/10000)\n",
            "  ✓ 7700 movies completed!\n",
            "  Scraping details: [███████████████████████████████░░░░░░░░░] 78.0% (7800/10000)\n",
            "  ✓ 7800 movies completed!\n",
            "  Scraping details: [███████████████████████████████░░░░░░░░░] 79.0% (7900/10000)\n",
            "  ✓ 7900 movies completed!\n",
            "  Scraping details: [████████████████████████████████░░░░░░░░] 80.0% (8000/10000)\n",
            "  ✓ 8000 movies completed!\n",
            "  Scraping details: [████████████████████████████████░░░░░░░░] 81.0% (8100/10000)\n",
            "  ✓ 8100 movies completed!\n",
            "  Scraping details: [████████████████████████████████░░░░░░░░] 82.0% (8200/10000)\n",
            "  ✓ 8200 movies completed!\n",
            "  Scraping details: [█████████████████████████████████░░░░░░░] 83.0% (8300/10000)\n",
            "  ✓ 8300 movies completed!\n",
            "  Scraping details: [█████████████████████████████████░░░░░░░] 84.0% (8400/10000)\n",
            "  ✓ 8400 movies completed!\n",
            "  Scraping details: [██████████████████████████████████░░░░░░] 85.0% (8500/10000)\n",
            "  ✓ 8500 movies completed!\n",
            "  Scraping details: [██████████████████████████████████░░░░░░] 86.0% (8600/10000)\n",
            "  ✓ 8600 movies completed!\n",
            "  Scraping details: [██████████████████████████████████░░░░░░] 87.0% (8700/10000)\n",
            "  ✓ 8700 movies completed!\n",
            "  Scraping details: [███████████████████████████████████░░░░░] 88.0% (8800/10000)\n",
            "  ✓ 8800 movies completed!\n",
            "  Scraping details: [███████████████████████████████████░░░░░] 89.0% (8900/10000)\n",
            "  ✓ 8900 movies completed!\n",
            "  Scraping details: [████████████████████████████████████░░░░] 90.0% (9000/10000)\n",
            "  ✓ 9000 movies completed!\n",
            "  Scraping details: [████████████████████████████████████░░░░] 91.0% (9100/10000)\n",
            "  ✓ 9100 movies completed!\n",
            "  Scraping details: [████████████████████████████████████░░░░] 92.0% (9200/10000)\n",
            "  ✓ 9200 movies completed!\n",
            "  Scraping details: [█████████████████████████████████████░░░] 93.0% (9300/10000)\n",
            "  ✓ 9300 movies completed!\n",
            "  Scraping details: [█████████████████████████████████████░░░] 94.0% (9400/10000)\n",
            "  ✓ 9400 movies completed!\n",
            "  Scraping details: [██████████████████████████████████████░░] 95.0% (9500/10000)\n",
            "  ✓ 9500 movies completed!\n",
            "  Scraping details: [██████████████████████████████████████░░] 96.0% (9600/10000)\n",
            "  ✓ 9600 movies completed!\n",
            "  Scraping details: [██████████████████████████████████████░░] 97.0% (9700/10000)\n",
            "  ✓ 9700 movies completed!\n",
            "  Scraping details: [███████████████████████████████████████░] 98.0% (9800/10000)\n",
            "  ✓ 9800 movies completed!\n",
            "  Scraping details: [███████████████████████████████████████░] 99.0% (9900/10000)\n",
            "  ✓ 9900 movies completed!\n",
            "  Scraping details: [████████████████████████████████████████] 100.0% (10000/10000)\n",
            "  ✓ 10000 movies completed!\n",
            "  Scraping details: [████████████████████████████████████████] 100.0% (10000/10000)\n",
            "\n",
            "[TMDB] ✓ Scraped 10000 movies with 17 features\n",
            "\n",
            "✓ SCRAPING COMPLETE!\n",
            "\n",
            "Output file: tmdb_data_17_features.csv\n",
            "Total movies: 10000\n",
            "Total features: 17\n",
            "\n",
            "17 Features collected:\n",
            "   1. title\n",
            "   2. release_date\n",
            "   3. tmdb_id\n",
            "   4. imdb_id\n",
            "   5. budget\n",
            "   6. revenue_tmdb\n",
            "   7. genres\n",
            "   8. runtime\n",
            "   9. country\n",
            "  10. mpaa_rating\n",
            "  11. popularity\n",
            "  12. is_franchise\n",
            "  13. director\n",
            "  14. actors\n",
            "  15. vote_average\n",
            "  16. production_companies\n",
            "  17. distributor_tmdb\n",
            "\n",
            "✓ Data saved to 'tmdb_data_17_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TMDB_CSV_FILE = \"tmdb_data_17_features.csv\"\n",
        "MIN_RELEASE_YEAR = 2000"
      ],
      "metadata": {
        "id": "n-vKvZjS1xwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HELPER FUNCTIONS\n"
      ],
      "metadata": {
        "id": "NRDrcBtoqOwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_money(text):\n",
        "    \"\"\"Convert '$123,456,789' to 123456789\"\"\"\n",
        "    try:\n",
        "        return int(re.sub(r'[^\\d]', '', text))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_number(text):\n",
        "    \"\"\"Convert '3,456' to 3456\"\"\"\n",
        "    try:\n",
        "        return int(re.sub(r'[^\\d]', '', text))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def print_progress_bar(current, total, success, failed):\n",
        "    \"\"\"Print a nice progress bar\"\"\"\n",
        "    percentage = (current / total) * 100\n",
        "    bar_length = 40\n",
        "    filled = int(bar_length * current / total)\n",
        "    bar = '█' * filled + '░' * (bar_length - filled)\n",
        "    print(f\"\\r  [{bar}] {percentage:.1f}% | Success: {success} | Failed: {failed}\", end='', flush=True)"
      ],
      "metadata": {
        "id": "dDDGqW2RqKtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Box Office Mojo URL\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j7JZxL5yqXMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bom_url(imdb_id):\n",
        "    \"\"\"Create Box Office Mojo URL from IMDB ID\"\"\"\n",
        "    if pd.isna(imdb_id) or not imdb_id:\n",
        "        return None\n",
        "\n",
        "    imdb_id = str(imdb_id).strip()\n",
        "\n",
        "    if not imdb_id.startswith('tt'):\n",
        "        return None\n",
        "\n",
        "    return f\"https://www.boxofficemojo.com/title/{imdb_id}/\""
      ],
      "metadata": {
        "id": "vp4dd1qGqYon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOX OFFICE MOJO PAGE SCRAPING\n",
        "\n"
      ],
      "metadata": {
        "id": "Bujx00iwqdrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_bom_page(url, headers):\n",
        "    \"\"\"Scrape data from a single Box Office Mojo movie page\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        # Check if page exists\n",
        "        if response.status_code == 404:\n",
        "            return None\n",
        "\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Check if page has box office data\n",
        "        has_data = soup.find('div', class_='a-section a-spacing-none mojo-performance-summary-table')\n",
        "        if not has_data:\n",
        "            return None\n",
        "\n",
        "        movie_data = {}\n",
        "\n",
        "        # Extract grosses from performance summary\n",
        "        perf_summary = soup.find('div', class_='mojo-performance-summary-table')\n",
        "        if perf_summary:\n",
        "            sections = perf_summary.find_all('div', class_='a-section')\n",
        "\n",
        "            for section in sections:\n",
        "                text = section.get_text()\n",
        "                money = section.find('span', class_='money')\n",
        "\n",
        "                if money:\n",
        "                    value = parse_money(money.get_text(strip=True))\n",
        "\n",
        "                    if 'Domestic' in text:\n",
        "                        movie_data['domestic_total_gross'] = value\n",
        "                    elif 'International' in text:\n",
        "                        movie_data['international_gross'] = value\n",
        "                    elif 'Worldwide' in text:\n",
        "                        movie_data['worldwide_gross'] = value\n",
        "\n",
        "        # Extract details from summary table\n",
        "        summary_table = soup.find('div', class_='mojo-summary-values')\n",
        "        if summary_table:\n",
        "            all_spans = summary_table.find_all('span')\n",
        "\n",
        "            i = 0\n",
        "            while i < len(all_spans):\n",
        "                label = all_spans[i].get_text(strip=True)\n",
        "\n",
        "                if i + 1 < len(all_spans):\n",
        "                    value_span = all_spans[i + 1]\n",
        "                    value_text = value_span.get_text(strip=True)\n",
        "\n",
        "                    if 'Distributor' in label:\n",
        "                        movie_data['distributor_bom'] = value_text\n",
        "\n",
        "                    elif 'Opening' in label:\n",
        "                        money = value_span.find('span', class_='money')\n",
        "                        if money:\n",
        "                            movie_data['opening_weekend_revenue'] = parse_money(money.get_text(strip=True))\n",
        "\n",
        "                        theaters_match = re.search(r'([\\d,]+)\\s+theaters?', value_text)\n",
        "                        if theaters_match:\n",
        "                            movie_data['opening_theaters_count'] = parse_number(theaters_match.group(1))\n",
        "\n",
        "                    elif 'Release Date' in label:\n",
        "                        movie_data['opening_date_bom'] = value_text\n",
        "\n",
        "                    elif 'MPAA' in label:\n",
        "                        movie_data['mpaa_rating_bom'] = value_text\n",
        "\n",
        "                    elif 'Running Time' in label:\n",
        "                        movie_data['runtime_bom'] = value_text\n",
        "\n",
        "                    elif 'Genres' in label:\n",
        "                        movie_data['genres_bom'] = value_text\n",
        "\n",
        "                    elif 'Widest Release' in label:\n",
        "                        theaters_match = re.search(r'([\\d,]+)\\s+theaters?', value_text)\n",
        "                        if theaters_match:\n",
        "                            movie_data['widest_release_theaters'] = parse_number(theaters_match.group(1))\n",
        "\n",
        "                i += 1\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        if 'opening_weekend_revenue' in movie_data and 'opening_theaters_count' in movie_data:\n",
        "            if movie_data['opening_theaters_count'] and movie_data['opening_theaters_count'] > 0:\n",
        "                movie_data['avg_per_theater_opening'] = movie_data['opening_weekend_revenue'] // movie_data['opening_theaters_count']\n",
        "\n",
        "        if 'opening_weekend_revenue' in movie_data and 'domestic_total_gross' in movie_data:\n",
        "            if movie_data['domestic_total_gross'] and movie_data['domestic_total_gross'] > 0:\n",
        "                movie_data['percent_of_total'] = round((movie_data['opening_weekend_revenue'] / movie_data['domestic_total_gross']) * 100, 2)\n",
        "\n",
        "        # Estimate opening week (opening weekend × 1.4)\n",
        "        if 'opening_weekend_revenue' in movie_data and movie_data['opening_weekend_revenue']:\n",
        "            movie_data['opening_week_revenue'] = int(movie_data['opening_weekend_revenue'] * 1.4)\n",
        "\n",
        "        return movie_data if movie_data else None\n",
        "\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "A2chyDkhqpTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOX OFFICE MOJO DATA COLLECTION\n"
      ],
      "metadata": {
        "id": "6oxJpTLGq2AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_bom_for_all_movies(tmdb_df):\n",
        "    \"\"\"Scrape Box Office Mojo for all movies in TMDB dataframe\"\"\"\n",
        "    print(\"SCRAPING BOX OFFICE MOJO\")\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[BOM] Scraping movies...\")\n",
        "\n",
        "    bom_data_list = []\n",
        "    success_count = 0\n",
        "    fail_count = 0\n",
        "\n",
        "    for idx, row in tmdb_df.iterrows():\n",
        "        title = row['title']\n",
        "        imdb_id = row.get('imdb_id', None)\n",
        "        current = idx + 1\n",
        "\n",
        "        # Check if we have IMDB ID\n",
        "        if pd.isna(imdb_id) or not imdb_id:\n",
        "            fail_count += 1\n",
        "            print_progress_bar(current, len(tmdb_df), success_count, fail_count)\n",
        "            continue\n",
        "\n",
        "        # Get BOM URL\n",
        "        movie_url = get_bom_url(imdb_id)\n",
        "\n",
        "        if movie_url:\n",
        "            # Scrape the movie page\n",
        "            movie_data = scrape_bom_page(movie_url, headers)\n",
        "\n",
        "            if movie_data:\n",
        "                movie_data['title'] = title\n",
        "                movie_data['bom_url'] = movie_url\n",
        "                bom_data_list.append(movie_data)\n",
        "                success_count += 1\n",
        "            else:\n",
        "                fail_count += 1\n",
        "        else:\n",
        "            fail_count += 1\n",
        "\n",
        "        # Show progress bar\n",
        "        print_progress_bar(current, len(tmdb_df), success_count, fail_count)\n",
        "\n",
        "        # Show milestone messages every 100 movies\n",
        "        if current % 100 == 0:\n",
        "            print(f\"\\n  ✓ {current} movies processed! (Success: {success_count}, Failed: {fail_count})\")\n",
        "            print_progress_bar(current, len(tmdb_df), success_count, fail_count)\n",
        "\n",
        "        # Wait between requests\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"\\n\\n[BOM] ✓ Successfully scraped {success_count} movies!\")\n",
        "    print(f\"[BOM] ✗ Failed to scrape {fail_count} movies\\n\")\n",
        "\n",
        "    return pd.DataFrame(bom_data_list)"
      ],
      "metadata": {
        "id": "YC_7qMQr1y8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA MERGING\n"
      ],
      "metadata": {
        "id": "t7nBQiKtq98n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def merge_datasets(tmdb_df, bom_df):\n",
        "    \"\"\"Merge TMDB and Box Office Mojo datasets\"\"\"\n",
        "    print(\"MERGING DATASETS\")\n",
        "\n",
        "    print(f\"\\n[MERGE] TMDB movies: {len(tmdb_df)}\")\n",
        "    print(f\"[MERGE] BOM movies: {len(bom_df)}\")\n",
        "\n",
        "    # Merge on title\n",
        "    merged = pd.merge(tmdb_df, bom_df, on='title', how='left')\n",
        "\n",
        "    # Count movies with BOM data\n",
        "    movies_with_bom = merged['domestic_total_gross'].notna().sum()\n",
        "    match_percentage = (movies_with_bom / len(tmdb_df)) * 100\n",
        "\n",
        "    print(f\"[MERGE] ✓ {movies_with_bom} movies have BOM data\")\n",
        "    print(f\"[MERGE] ✓ Match rate: {match_percentage:.1f}%\")\n",
        "    print(f\"[MERGE] ✓ Total features: {len(merged.columns)}\\n\")\n",
        "\n",
        "    return merged\n"
      ],
      "metadata": {
        "id": "_uYVz989q6dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BOM PIPELINE\n"
      ],
      "metadata": {
        "id": "-e4pc525rIie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIMIT_ROWS = None\n",
        "print(\"BOX OFFICE MOJO SCRAPER\")\n",
        "\n",
        "# Load TMDB data\n",
        "print(f\"[LOAD] Loading TMDB data from '{TMDB_CSV_FILE}'...\")\n",
        "\n",
        "try:\n",
        "    tmdb_data = pd.read_csv(TMDB_CSV_FILE)\n",
        "    print(f\"[LOAD] ✓ Loaded {len(tmdb_data)} movies!\\n\")\n",
        "\n",
        "    # Filter by release year\n",
        "    if MIN_RELEASE_YEAR:\n",
        "        tmdb_data['release_year'] = pd.to_datetime(tmdb_data['release_date'], errors='coerce').dt.year\n",
        "        before = len(tmdb_data)\n",
        "        tmdb_data = tmdb_data[tmdb_data['release_year'] >= MIN_RELEASE_YEAR]\n",
        "        print(f\"[LOAD] ⚠ Filtered to movies from {MIN_RELEASE_YEAR}+: {len(tmdb_data)} movies\\n\")\n",
        "\n",
        "    # Limit rows if specified\n",
        "    if LIMIT_ROWS:\n",
        "        tmdb_data = tmdb_data.head(LIMIT_ROWS)\n",
        "        print(f\"[LOAD] ⚠ Limited to first {LIMIT_ROWS} movies for testing\\n\")\n",
        "\n",
        "    # Scrape Box Office Mojo\n",
        "    bom_data = scrape_bom_for_all_movies(tmdb_data)\n",
        "\n",
        "    # Merge datasets\n",
        "    final_data = merge_datasets(tmdb_data, bom_data)\n",
        "\n",
        "    # Save to CSV\n",
        "    output_file = 'movie_dataset_with_bom.csv'\n",
        "    final_data.to_csv(output_file, index=False)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"✓ SCRAPING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTotal movies: {len(final_data)}\")\n",
        "    print(f\"Movies with BOM data: {final_data['domestic_total_gross'].notna().sum()}\")\n",
        "    print(f\"Total features: {len(final_data.columns)}\")\n",
        "    print(f\"Saved to: {output_file}\\n\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"Sample of the data:\")\n",
        "    print(final_data[['title', 'release_date', 'domestic_total_gross', 'opening_weekend_revenue']].head(5))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"[ERROR] File '{TMDB_CSV_FILE}' not found!\")\n",
        "    print(\"[ERROR] Please run Step 1 first to generate the TMDB data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy2Jgri-2HAu",
        "outputId": "351cd585-2cda-450d-89d2-9b3ca397dbba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOX OFFICE MOJO SCRAPER\n",
            "[LOAD] Loading TMDB data from 'tmdb_data_17_features.csv'...\n",
            "[LOAD] ✓ Loaded 10000 movies!\n",
            "\n",
            "[LOAD] ⚠ Filtered to movies from 2000+: 7149 movies\n",
            "\n",
            "SCRAPING BOX OFFICE MOJO\n",
            "\n",
            "[BOM] Scraping movies...\n",
            "  [░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.4% | Success: 94 | Failed: 0\n",
            "  ✓ 100 movies processed! (Success: 94, Failed: 0)\n",
            "  [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.8% | Success: 180 | Failed: 0\n",
            "  ✓ 200 movies processed! (Success: 180, Failed: 0)\n",
            "  [█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.2% | Success: 261 | Failed: 0\n",
            "  ✓ 300 movies processed! (Success: 261, Failed: 0)\n",
            "  [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.0% | Success: 421 | Failed: 0\n",
            "  ✓ 500 movies processed! (Success: 421, Failed: 0)\n",
            "  [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8.4% | Success: 499 | Failed: 0\n",
            "  ✓ 600 movies processed! (Success: 499, Failed: 0)\n",
            "  [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.8% | Success: 578 | Failed: 0\n",
            "  ✓ 700 movies processed! (Success: 578, Failed: 0)\n",
            "  [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.2% | Success: 656 | Failed: 0\n",
            "  ✓ 800 movies processed! (Success: 656, Failed: 0)\n",
            "  [█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.0% | Success: 809 | Failed: 0\n",
            "  ✓ 1000 movies processed! (Success: 809, Failed: 0)\n",
            "  [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.4% | Success: 887 | Failed: 0\n",
            "  ✓ 1100 movies processed! (Success: 887, Failed: 0)\n",
            "  [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.8% | Success: 963 | Failed: 0\n",
            "  ✓ 1200 movies processed! (Success: 963, Failed: 0)\n",
            "  [███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19.6% | Success: 1124 | Failed: 0\n",
            "  ✓ 1400 movies processed! (Success: 1124, Failed: 0)\n",
            "  [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22.4% | Success: 1263 | Failed: 0\n",
            "  ✓ 1600 movies processed! (Success: 1263, Failed: 0)\n",
            "  [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.2% | Success: 1411 | Failed: 0\n",
            "  ✓ 1800 movies processed! (Success: 1411, Failed: 0)\n",
            "  [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26.6% | Success: 1482 | Failed: 0\n",
            "  ✓ 1900 movies processed! (Success: 1482, Failed: 0)\n",
            "  [███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28.0% | Success: 1551 | Failed: 0\n",
            "  ✓ 2000 movies processed! (Success: 1551, Failed: 0)\n",
            "  [███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.4% | Success: 1629 | Failed: 0\n",
            "  ✓ 2100 movies processed! (Success: 1629, Failed: 0)\n",
            "  [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.8% | Success: 1691 | Failed: 0\n",
            "  ✓ 2200 movies processed! (Success: 1691, Failed: 0)\n",
            "  [█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33.6% | Success: 1831 | Failed: 0\n",
            "  ✓ 2400 movies processed! (Success: 1831, Failed: 0)\n",
            "  [█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 35.0% | Success: 1908 | Failed: 0\n",
            "  ✓ 2500 movies processed! (Success: 1908, Failed: 0)\n",
            "  [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.4% | Success: 1980 | Failed: 0\n",
            "  ✓ 2600 movies processed! (Success: 1980, Failed: 0)\n",
            "  [███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 37.8% | Success: 2050 | Failed: 0\n",
            "  ✓ 2700 movies processed! (Success: 2050, Failed: 0)\n",
            "  [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40.6% | Success: 2185 | Failed: 0\n",
            "  ✓ 2900 movies processed! (Success: 2185, Failed: 0)\n",
            "  [█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.4% | Success: 2330 | Failed: 0\n",
            "  ✓ 3100 movies processed! (Success: 2330, Failed: 0)\n",
            "  [█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44.8% | Success: 2398 | Failed: 0\n",
            "  ✓ 3200 movies processed! (Success: 2398, Failed: 0)\n",
            "  [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.2% | Success: 2473 | Failed: 0\n",
            "  ✓ 3300 movies processed! (Success: 2473, Failed: 0)\n",
            "  [███████████████████░░░░░░░░░░░░░░░░░░░░░] 49.0% | Success: 2614 | Failed: 0\n",
            "  ✓ 3500 movies processed! (Success: 2614, Failed: 0)\n",
            "  [████████████████████░░░░░░░░░░░░░░░░░░░░] 50.4% | Success: 2681 | Failed: 0\n",
            "  ✓ 3600 movies processed! (Success: 2681, Failed: 0)\n",
            "  [████████████████████░░░░░░░░░░░░░░░░░░░░] 51.8% | Success: 2748 | Failed: 0\n",
            "  ✓ 3700 movies processed! (Success: 2748, Failed: 0)\n",
            "  [█████████████████████░░░░░░░░░░░░░░░░░░░] 54.6% | Success: 2884 | Failed: 0\n",
            "  ✓ 3900 movies processed! (Success: 2884, Failed: 0)\n",
            "  [██████████████████████░░░░░░░░░░░░░░░░░░] 56.0% | Success: 2947 | Failed: 0\n",
            "  ✓ 4000 movies processed! (Success: 2947, Failed: 0)\n",
            "  [███████████████████████░░░░░░░░░░░░░░░░░] 58.7% | Success: 3091 | Failed: 0\n",
            "  ✓ 4200 movies processed! (Success: 3091, Failed: 0)\n",
            "  [████████████████████████░░░░░░░░░░░░░░░░] 61.5% | Success: 3235 | Failed: 0\n",
            "  ✓ 4400 movies processed! (Success: 3235, Failed: 0)\n",
            "  [█████████████████████████░░░░░░░░░░░░░░░] 62.9% | Success: 3303 | Failed: 0\n",
            "  ✓ 4500 movies processed! (Success: 3303, Failed: 0)\n",
            "  [█████████████████████████░░░░░░░░░░░░░░░] 64.3% | Success: 3369 | Failed: 0\n",
            "  ✓ 4600 movies processed! (Success: 3369, Failed: 0)\n",
            "  [██████████████████████████░░░░░░░░░░░░░░] 67.1% | Success: 3511 | Failed: 0\n",
            "  ✓ 4800 movies processed! (Success: 3511, Failed: 0)\n",
            "  [███████████████████████████░░░░░░░░░░░░░] 69.9% | Success: 3648 | Failed: 0\n",
            "  ✓ 5000 movies processed! (Success: 3648, Failed: 0)\n",
            "  [████████████████████████████░░░░░░░░░░░░] 71.3% | Success: 3714 | Failed: 0\n",
            "  ✓ 5100 movies processed! (Success: 3714, Failed: 0)\n",
            "  [█████████████████████████████░░░░░░░░░░░] 72.7% | Success: 3782 | Failed: 0\n",
            "  ✓ 5200 movies processed! (Success: 3782, Failed: 0)\n",
            "  [██████████████████████████████░░░░░░░░░░] 75.5% | Success: 3934 | Failed: 0\n",
            "  ✓ 5400 movies processed! (Success: 3934, Failed: 0)\n",
            "  [██████████████████████████████░░░░░░░░░░] 76.9% | Success: 4010 | Failed: 0\n",
            "  ✓ 5500 movies processed! (Success: 4010, Failed: 0)\n",
            "  [███████████████████████████████░░░░░░░░░] 78.3% | Success: 4081 | Failed: 0\n",
            "  ✓ 5600 movies processed! (Success: 4081, Failed: 0)\n",
            "  [███████████████████████████████░░░░░░░░░] 79.7% | Success: 4157 | Failed: 0\n",
            "  ✓ 5700 movies processed! (Success: 4157, Failed: 0)\n",
            "  [████████████████████████████████░░░░░░░░] 81.1% | Success: 4227 | Failed: 0\n",
            "  ✓ 5800 movies processed! (Success: 4227, Failed: 0)\n",
            "  [█████████████████████████████████░░░░░░░] 82.5% | Success: 4295 | Failed: 0\n",
            "  ✓ 5900 movies processed! (Success: 4295, Failed: 0)\n",
            "  [█████████████████████████████████░░░░░░░] 83.9% | Success: 4374 | Failed: 0\n",
            "  ✓ 6000 movies processed! (Success: 4374, Failed: 0)\n",
            "  [██████████████████████████████████░░░░░░] 85.3% | Success: 4448 | Failed: 0\n",
            "  ✓ 6100 movies processed! (Success: 4448, Failed: 0)\n",
            "  [██████████████████████████████████░░░░░░] 86.7% | Success: 4517 | Failed: 0\n",
            "  ✓ 6200 movies processed! (Success: 4517, Failed: 0)\n",
            "  [███████████████████████████████████░░░░░] 88.1% | Success: 4588 | Failed: 0\n",
            "  ✓ 6300 movies processed! (Success: 4588, Failed: 0)\n",
            "  [████████████████████████████████████░░░░] 90.9% | Success: 4734 | Failed: 0\n",
            "  ✓ 6500 movies processed! (Success: 4734, Failed: 0)\n",
            "  [███████████████████████████████████████░] 97.9% | Success: 5081 | Failed: 0\n",
            "  ✓ 7000 movies processed! (Success: 5081, Failed: 0)\n",
            "  [███████████████████████████████████████░] 99.3% | Success: 5151 | Failed: 0\n",
            "  ✓ 7100 movies processed! (Success: 5151, Failed: 0)\n",
            "  [████████████████████████████████████████] 100.7% | Success: 5212 | Failed: 0\n",
            "  ✓ 7200 movies processed! (Success: 5212, Failed: 0)\n",
            "  [████████████████████████████████████████] 102.1% | Success: 5280 | Failed: 0\n",
            "  ✓ 7300 movies processed! (Success: 5280, Failed: 0)\n",
            "  [█████████████████████████████████████████] 104.9% | Success: 5429 | Failed: 0\n",
            "  ✓ 7500 movies processed! (Success: 5429, Failed: 0)\n",
            "  [██████████████████████████████████████████] 106.3% | Success: 5497 | Failed: 0\n",
            "  ✓ 7600 movies processed! (Success: 5497, Failed: 0)\n",
            "  [███████████████████████████████████████████] 107.7% | Success: 5572 | Failed: 0\n",
            "  ✓ 7700 movies processed! (Success: 5572, Failed: 0)\n",
            "  [███████████████████████████████████████████] 109.1% | Success: 5642 | Failed: 0\n",
            "  ✓ 7800 movies processed! (Success: 5642, Failed: 0)\n",
            "  [████████████████████████████████████████████] 110.5% | Success: 5714 | Failed: 0\n",
            "  ✓ 7900 movies processed! (Success: 5714, Failed: 0)\n",
            "  [████████████████████████████████████████████] 111.9% | Success: 5778 | Failed: 0\n",
            "  ✓ 8000 movies processed! (Success: 5778, Failed: 0)\n",
            "  [█████████████████████████████████████████████] 114.7% | Success: 5931 | Failed: 0\n",
            "  ✓ 8200 movies processed! (Success: 5931, Failed: 0)\n",
            "  [██████████████████████████████████████████████] 117.5% | Success: 6052 | Failed: 0\n",
            "  ✓ 8400 movies processed! (Success: 6052, Failed: 0)\n",
            "  [███████████████████████████████████████████████] 118.9% | Success: 6125 | Failed: 0\n",
            "  ✓ 8500 movies processed! (Success: 6125, Failed: 0)\n",
            "  [████████████████████████████████████████████████] 121.7% | Success: 6255 | Failed: 1\n",
            "  ✓ 8700 movies processed! (Success: 6255, Failed: 1)\n",
            "  [█████████████████████████████████████████████████] 124.5% | Success: 6387 | Failed: 1\n",
            "  ✓ 8900 movies processed! (Success: 6387, Failed: 1)\n",
            "  [██████████████████████████████████████████████████] 125.9% | Success: 6463 | Failed: 1\n",
            "  ✓ 9000 movies processed! (Success: 6463, Failed: 1)\n",
            "  [███████████████████████████████████████████████████] 128.7% | Success: 6613 | Failed: 2\n",
            "  ✓ 9200 movies processed! (Success: 6613, Failed: 2)\n",
            "  [████████████████████████████████████████████████████] 131.5% | Success: 6745 | Failed: 2\n",
            "  ✓ 9400 movies processed! (Success: 6745, Failed: 2)\n",
            "  [█████████████████████████████████████████████████████] 132.9% | Success: 6810 | Failed: 2\n",
            "  ✓ 9500 movies processed! (Success: 6810, Failed: 2)\n",
            "  [██████████████████████████████████████████████████████] 135.7% | Success: 6944 | Failed: 2\n",
            "  ✓ 9700 movies processed! (Success: 6944, Failed: 2)\n",
            "  [██████████████████████████████████████████████████████] 137.1% | Success: 7009 | Failed: 2\n",
            "  ✓ 9800 movies processed! (Success: 7009, Failed: 2)\n",
            "  [███████████████████████████████████████████████████████] 138.5% | Success: 7075 | Failed: 2\n",
            "  ✓ 9900 movies processed! (Success: 7075, Failed: 2)\n",
            "  [███████████████████████████████████████████████████████] 139.9% | Success: 7147 | Failed: 2\n",
            "  ✓ 10000 movies processed! (Success: 7147, Failed: 2)\n",
            "  [███████████████████████████████████████████████████████] 139.9% | Success: 7147 | Failed: 2\n",
            "\n",
            "[BOM] ✓ Successfully scraped 7147 movies!\n",
            "[BOM] ✗ Failed to scrape 2 movies\n",
            "\n",
            "MERGING DATASETS\n",
            "\n",
            "[MERGE] TMDB movies: 7149\n",
            "[MERGE] BOM movies: 7147\n",
            "[MERGE] ✓ 4085 movies have BOM data\n",
            "[MERGE] ✓ Match rate: 57.1%\n",
            "[MERGE] ✓ Total features: 30\n",
            "\n",
            "============================================================\n",
            "✓ SCRAPING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Total movies: 7385\n",
            "Movies with BOM data: 4085\n",
            "Total features: 30\n",
            "Saved to: movie_dataset_with_bom.csv\n",
            "\n",
            "Sample of the data:\n",
            "                                    title release_date  domestic_total_gross  \\\n",
            "0                              Zootopia 2   2025-11-26           258970004.0   \n",
            "1                         The Running Man   2025-11-11            37605731.0   \n",
            "2  Wake Up Dead Man: A Knives Out Mystery   2025-11-26             1600000.0   \n",
            "3                              TRON: Ares   2025-10-08            73161014.0   \n",
            "4               Five Nights at Freddy's 2   2025-12-03            95481015.0   \n",
            "\n",
            "   opening_weekend_revenue  \n",
            "0              100262540.0  \n",
            "1               16495564.0  \n",
            "2                 600000.0  \n",
            "3               33241433.0  \n",
            "4               64007430.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the shape of the new dataset"
      ],
      "metadata": {
        "id": "OeJHqHAVrMaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/movie_dataset_with_bom.csv')\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnJaipFgoyRN",
        "outputId": "3c14e737-452b-46ae-d557-b4c7517f176e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7385, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING DATA AND DATA CLEANING\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLb2iBd30Lyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_data(filepath, target_col='opening_week_revenue'):\n",
        "    \"\"\"Load data and handle missing values intelligently\"\"\"\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Remove columns that contain future information (prevent data leakage)\n",
        "    leakage_columns = [\n",
        "        'revenue_tmdb',\n",
        "        'domestic_total_gross',\n",
        "        'international_gross',\n",
        "        'worldwide_gross',\n",
        "        'opening_weekend_revenue',\n",
        "        'percent_of_total'\n",
        "    ]\n",
        "\n",
        "    for col in leakage_columns:\n",
        "        if col in df.columns:\n",
        "            df = df.drop(columns=[col])\n",
        "\n",
        "    # Drop rows where target is missing\n",
        "    df = df[df[target_col].notna()]\n",
        "\n",
        "    # Drop duplicate movies\n",
        "    df = df.drop_duplicates(subset=['tmdb_id'], keep='first')\n",
        "\n",
        "    # Impute numeric features using median\n",
        "    numeric_features = ['budget', 'runtime', 'popularity', 'vote_average']\n",
        "    for col in numeric_features:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # Binary feature\n",
        "    if 'is_franchise' in df.columns:\n",
        "        df['is_franchise'] = df['is_franchise'].fillna(df['is_franchise'].mode()[0])\n",
        "\n",
        "    # Categorical features\n",
        "    if 'country' in df.columns:\n",
        "        df['country'] = df['country'].fillna('US')\n",
        "\n",
        "    if 'mpaa_rating' in df.columns:\n",
        "        df['mpaa_rating'] = df['mpaa_rating'].fillna('NR')\n",
        "\n",
        "    if 'genres' in df.columns:\n",
        "        df['genres'] = df['genres'].fillna('Drama')\n",
        "\n",
        "    if 'director' in df.columns:\n",
        "        df['director'] = df['director'].fillna('Unknown Director')\n",
        "\n",
        "    if 'actors' in df.columns:\n",
        "        df['actors'] = df['actors'].fillna('')\n",
        "\n",
        "    if 'production_companies' in df.columns:\n",
        "        df['production_companies'] = df['production_companies'].fillna('Independent')\n",
        "\n",
        "    if 'distributor_bom' in df.columns:\n",
        "        df['distributor_bom'] = df['distributor_bom'].fillna('Independent')\n",
        "\n",
        "    if 'release_date' in df.columns:\n",
        "        if 'release_year' in df.columns:\n",
        "            df['release_date'] = df.apply(\n",
        "                lambda row: f\"{row['release_year']}-07-01\"\n",
        "                if pd.isna(row['release_date']) and pd.notna(row['release_year'])\n",
        "                else row['release_date'],\n",
        "                axis=1\n",
        "            )\n",
        "        df = df[df['release_date'].notna()]\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "v6xRoNDnuC6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURE ENGINEERING\n"
      ],
      "metadata": {
        "id": "1iXIxwMO0WXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_basic_features(df):\n",
        "    \"\"\"Create basic features from existing columns\"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. DATE FEATURES\n",
        "    df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
        "    df['release_month'] = df['release_date'].dt.month\n",
        "    df['release_day'] = df['release_date'].dt.day\n",
        "    df['release_day_of_week'] = df['release_date'].dt.dayofweek\n",
        "    df['release_quarter'] = df['release_date'].dt.quarter\n",
        "\n",
        "    # Holiday indicators\n",
        "    df['is_summer'] = df['release_month'].isin([5, 6, 7, 8]).astype(int)\n",
        "    df['is_holiday_season'] = df['release_month'].isin([11, 12]).astype(int)\n",
        "    df['is_weekend_release'] = df['release_day_of_week'].isin([4, 5]).astype(int)\n",
        "\n",
        "    # 2. GENRE FEATURES\n",
        "    if 'genres' in df.columns:\n",
        "        top_genres = [\n",
        "            'Action', 'Adventure', 'Comedy', 'Drama', 'Thriller',\n",
        "            'Horror', 'Science Fiction', 'Fantasy', 'Animation'\n",
        "        ]\n",
        "\n",
        "        for genre in top_genres:\n",
        "            col_name = f'genre_{genre.lower().replace(\" \", \"_\")}'\n",
        "            df[col_name] = df['genres'].apply(\n",
        "                lambda x: 1 if isinstance(x, str) and genre in x else 0\n",
        "            )\n",
        "\n",
        "        df['genre_count'] = df['genres'].apply(\n",
        "            lambda x: len(x.split('|')) if isinstance(x, str) and x else 1\n",
        "        )\n",
        "\n",
        "    # 3. ACTOR COUNT\n",
        "    if 'actors' in df.columns:\n",
        "        df['actor_count'] = df['actors'].apply(\n",
        "            lambda x: len(x.split('|')) if isinstance(x, str) and x else 0\n",
        "        )\n",
        "\n",
        "    # 4. MAJOR STUDIO\n",
        "    if 'production_companies' in df.columns:\n",
        "        major_studios = [\n",
        "            'Warner Bros', 'Universal', 'Disney', 'Paramount',\n",
        "            'Sony', '20th Century', 'Columbia', 'Marvel', 'Lucasfilm'\n",
        "        ]\n",
        "\n",
        "        df['is_major_studio'] = df['production_companies'].apply(\n",
        "            lambda x: 1 if isinstance(x, str) and any(studio in x for studio in major_studios) else 0\n",
        "        )\n",
        "\n",
        "    # 5. DIRECTOR FREQUENCY\n",
        "    if 'director' in df.columns:\n",
        "        director_freq = df['director'].value_counts()\n",
        "        df['director_movie_count'] = df['director'].map(director_freq).fillna(1)\n",
        "\n",
        "    # 6. BUDGET CATEGORIES\n",
        "    if 'budget' in df.columns:\n",
        "        df['is_blockbuster'] = (df['budget'] >= 100000000).astype(int)\n",
        "        df['is_mega_blockbuster'] = (df['budget'] >= 200000000).astype(int)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "RSpsH_iYuGII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADVANCED FEATURE ENGINEERING\n"
      ],
      "metadata": {
        "id": "bhOFglHf0a1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_features(df):\n",
        "    \"\"\"Create advanced engineered features\"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. LOG TRANSFORMATIONS\n",
        "    if 'budget' in df.columns:\n",
        "        df['log_budget'] = np.log10(df['budget'] + 1)\n",
        "\n",
        "    if 'popularity' in df.columns:\n",
        "        df['log_popularity'] = np.log10(df['popularity'] + 1)\n",
        "\n",
        "    if 'runtime' in df.columns:\n",
        "        df['log_runtime'] = np.log10(df['runtime'] + 1)\n",
        "\n",
        "    # 2. INTERACTION FEATURES\n",
        "    if 'budget' in df.columns:\n",
        "        if 'genre_action' in df.columns:\n",
        "            df['budget_action_ratio'] = df['budget'] * df['genre_action']\n",
        "\n",
        "        if 'genre_science_fiction' in df.columns:\n",
        "            df['budget_scifi_ratio'] = df['budget'] * df['genre_science_fiction']\n",
        "\n",
        "        if 'is_summer' in df.columns:\n",
        "            df['budget_summer_boost'] = df['budget'] * df['is_summer']\n",
        "\n",
        "        if 'is_holiday_season' in df.columns:\n",
        "            df['budget_holiday_boost'] = df['budget'] * df['is_holiday_season']\n",
        "\n",
        "    # 3. FRANCHISE × BUDGET\n",
        "    if 'is_franchise' in df.columns and 'budget' in df.columns:\n",
        "        df['franchise_budget'] = df['is_franchise'] * df['budget']\n",
        "\n",
        "    # 4. GENRE COMBINATIONS\n",
        "    if 'genre_action' in df.columns and 'genre_science_fiction' in df.columns:\n",
        "        df['action_scifi'] = df['genre_action'] * df['genre_science_fiction']\n",
        "\n",
        "    if 'genre_action' in df.columns and 'genre_adventure' in df.columns:\n",
        "        df['action_adventure'] = df['genre_action'] * df['genre_adventure']\n",
        "\n",
        "    # 5. POLYNOMIAL FEATURES\n",
        "    if 'budget' in df.columns:\n",
        "        df['budget_squared'] = df['budget'] ** 2\n",
        "\n",
        "    if 'runtime' in df.columns:\n",
        "        df['runtime_squared'] = df['runtime'] ** 2\n",
        "\n",
        "    # 6. RATIO FEATURES\n",
        "    if 'budget' in df.columns and 'genre_count' in df.columns:\n",
        "        df['budget_per_genre'] = df['budget'] / (df['genre_count'] + 1)\n",
        "\n",
        "    if 'budget' in df.columns and 'runtime' in df.columns:\n",
        "        df['budget_per_minute'] = df['budget'] / (df['runtime'] + 1)\n",
        "\n",
        "    if 'popularity' in df.columns and 'vote_average' in df.columns:\n",
        "        df['popularity_per_rating'] = df['popularity'] / (df['vote_average'] + 1)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "bVtrKDGiuIuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENCODING FEATURES\n"
      ],
      "metadata": {
        "id": "8xvH7Uhv0g3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_categorical_features(df, target_encodings=None, target='opening_week_revenue'):\n",
        "    \"\"\"Encode categorical features\"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Store target encodings for test/avatar data\n",
        "    if target_encodings is None:\n",
        "        target_encodings = {}\n",
        "\n",
        "    # 1. MPAA RATING - Ordinal\n",
        "    if 'mpaa_rating' in df.columns:\n",
        "        mpaa_order = {'G': 1, 'PG': 2, 'PG-13': 3, 'R': 4, 'NC-17': 5, 'NR': 0}\n",
        "        df['mpaa_rating_encoded'] = df['mpaa_rating'].map(mpaa_order).fillna(0)\n",
        "\n",
        "    # 2. COUNTRY - Target Encoding\n",
        "    if 'country' in df.columns:\n",
        "        if 'country' not in target_encodings and target in df.columns:\n",
        "            country_means = df.groupby('country')[target].mean()\n",
        "            target_encodings['country'] = country_means\n",
        "\n",
        "        if 'country' in target_encodings:\n",
        "            df['country_encoded'] = df['country'].map(target_encodings['country'])\n",
        "            df['country_encoded'] = df['country_encoded'].fillna(target_encodings['country'].mean())\n",
        "\n",
        "    # 3. DISTRIBUTOR - Target Encoding\n",
        "    if 'distributor_bom' in df.columns:\n",
        "        if 'distributor' not in target_encodings and target in df.columns:\n",
        "            distributor_means = df.groupby('distributor_bom')[target].mean()\n",
        "            target_encodings['distributor'] = distributor_means\n",
        "\n",
        "        if 'distributor' in target_encodings:\n",
        "            df['distributor_encoded'] = df['distributor_bom'].map(target_encodings['distributor'])\n",
        "            df['distributor_encoded'] = df['distributor_encoded'].fillna(target_encodings['distributor'].mean())\n",
        "\n",
        "    # Drop original text and ID columns\n",
        "    text_cols = [\n",
        "        'title', 'genres', 'actors', 'director', 'production_companies',\n",
        "        'release_date', 'mpaa_rating', 'country', 'distributor_bom',\n",
        "        'tmdb_id', 'imdb_id', 'distributor_tmdb', 'opening_date_bom',\n",
        "        'mpaa_rating_bom', 'runtime_bom', 'genres_bom', 'bom_url', 'release_year'\n",
        "    ]\n",
        "\n",
        "    cols_to_drop = [col for col in text_cols if col in df.columns]\n",
        "    df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "    return df, target_encodings\n"
      ],
      "metadata": {
        "id": "X6rlNQH9uLtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURE PIPELINE\n"
      ],
      "metadata": {
        "id": "6LNrj66-0k0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_feature_pipeline(df, target_encodings=None, is_training=True):\n",
        "    \"\"\"Apply complete feature engineering pipeline\"\"\"\n",
        "\n",
        "    # Apply all feature engineering steps\n",
        "    df = create_basic_features(df)\n",
        "    df = create_advanced_features(df)\n",
        "    df, target_encodings = encode_categorical_features(df, target_encodings)\n",
        "\n",
        "    # Fill any remaining missing values with median\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    return df, target_encodings\n"
      ],
      "metadata": {
        "id": "XNTH8LZjuN6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL INITIALIZATION\n"
      ],
      "metadata": {
        "id": "iGoz_Chq0olf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_all_models():\n",
        "    \"\"\"Initialize selected regression models\"\"\"\n",
        "\n",
        "    models = {\n",
        "        # LINEAR MODEL\n",
        "        'Linear Regression': LinearRegression(),\n",
        "\n",
        "        # TREE MODEL\n",
        "        'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
        "\n",
        "        # BOOSTING MODELS\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=150, max_depth=8,\n",
        "                                                       learning_rate=0.1, random_state=42),\n",
        "        'AdaBoost': AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "        'XGBoost': XGBRegressor(n_estimators=150, max_depth=8, learning_rate=0.1,\n",
        "                               random_state=42, n_jobs=-1, verbosity=0),\n",
        "        'LightGBM': LGBMRegressor(n_estimators=150, max_depth=8, learning_rate=0.1,\n",
        "                                 random_state=42, n_jobs=-1, verbose=-1, force_row_wise=True),\n",
        "\n",
        "        # OTHER MODELS\n",
        "        'SVR': SVR(kernel='rbf', C=1.0),\n",
        "        'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500,\n",
        "                                      random_state=42, early_stopping=True)\n",
        "    }\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "wyf6NYnCuQlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL TRAINING\n"
      ],
      "metadata": {
        "id": "QZgfji6E0sM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_all_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate all models\"\"\"\n",
        "\n",
        "    models = initialize_all_models()\n",
        "    results = []\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            # Train model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Predict\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Evaluate\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            results.append({\n",
        "                'Model': name,\n",
        "                'MAE': mae,\n",
        "                'RMSE': rmse,\n",
        "                'R2': r2,\n",
        "                'model_object': model\n",
        "            })\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame and sort by R²\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values('R2', ascending=False)\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "g7mqbN95uTFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL COMPARISON\n"
      ],
      "metadata": {
        "id": "C487bwb00wDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_model_comparison(results_df):\n",
        "    \"\"\"Display comparison of all models\"\"\"\n",
        "\n",
        "    print(\"MODEL COMPARISON (Ranked by R2)\\n\")\n",
        "    print(f\"{'Rank':<6} {'Model':<20} {'R2':<10} {'MAE':<15} {'RMSE':<15}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for i, (_, row) in enumerate(results_df.iterrows(), start=1):\n",
        "        print(\n",
        "            f\"{i:<6} \"\n",
        "            f\"{row['Model']:<20} \"\n",
        "            f\"{row['R2']:<10.4f} \"\n",
        "            f\"${row['MAE']:<14,.0f} \"\n",
        "            f\"${row['RMSE']:<14,.0f}\"\n",
        "        )\n",
        "\n",
        "    best_model = results_df.iloc[0]\n",
        "\n",
        "    print(\"\\nBest Model:\")\n",
        "    print(f\"Model: {best_model['Model']}\")\n",
        "    print(f\"R2: {best_model['R2']:.4f}\")\n",
        "    print(f\"MAE: ${best_model['MAE']:,.0f}\")\n",
        "    print(f\"RMSE: ${best_model['RMSE']:,.0f}\")\n",
        "\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "MdllM_fuuVOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AVATAR DATA PREPARATION\n"
      ],
      "metadata": {
        "id": "nvyiBDdj1BGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_avatar_data(avatar_csv_path, target_encodings):\n",
        "    \"\"\"Load and prepare Avatar data\"\"\"\n",
        "\n",
        "    avatar_df = pd.read_csv(avatar_csv_path)\n",
        "\n",
        "    # Handle missing numeric values\n",
        "    numeric_features = ['budget', 'runtime', 'popularity', 'vote_average']\n",
        "    for col in numeric_features:\n",
        "        if col in avatar_df.columns and avatar_df[col].isnull().sum() > 0:\n",
        "            avatar_df[col] = avatar_df[col].fillna(avatar_df[col].median())\n",
        "\n",
        "    if 'is_franchise' in avatar_df.columns:\n",
        "        avatar_df['is_franchise'] = avatar_df['is_franchise'].fillna(1)\n",
        "\n",
        "    if 'country' in avatar_df.columns:\n",
        "        avatar_df['country'] = avatar_df['country'].fillna('US')\n",
        "\n",
        "    if 'mpaa_rating' in avatar_df.columns:\n",
        "        avatar_df['mpaa_rating'] = avatar_df['mpaa_rating'].fillna('PG-13')\n",
        "\n",
        "    if 'genres' in avatar_df.columns:\n",
        "        avatar_df['genres'] = avatar_df['genres'].fillna('Action|Adventure')\n",
        "\n",
        "    avatar_df, _ = apply_feature_pipeline(\n",
        "        avatar_df, target_encodings, is_training=False\n",
        "    )\n",
        "\n",
        "    return avatar_df\n"
      ],
      "metadata": {
        "id": "obQFgmPY08eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avatar: Fire and Ash PREDICTION\n"
      ],
      "metadata": {
        "id": "m1jMvTzk077i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_avatar_with_best_model(best_model_row, scaler, avatar_df, training_columns):\n",
        "    \"\"\"Predict using the best performing model\"\"\"\n",
        "\n",
        "    best_model = best_model_row['model_object']\n",
        "\n",
        "    # Add missing columns\n",
        "    for col in training_columns:\n",
        "        if col not in avatar_df.columns:\n",
        "            avatar_df[col] = 0\n",
        "\n",
        "    # Drop extra columns\n",
        "    avatar_df = avatar_df[training_columns]\n",
        "\n",
        "    # Scale and predict\n",
        "    avatar_scaled = scaler.transform(avatar_df)\n",
        "    prediction = best_model.predict(avatar_scaled)[0]\n",
        "\n",
        "    return max(prediction, 0)\n"
      ],
      "metadata": {
        "id": "gEVM9-o_uX_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DATA = \"movie_dataset_with_bom.csv\""
      ],
      "metadata": {
        "id": "0Ymj1FGgufEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPARATION FOR MODELS\n"
      ],
      "metadata": {
        "id": "5k7ICB9j61NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_and_clean_data(TRAINING_DATA)\n",
        "df, target_encodings = apply_feature_pipeline(df)\n",
        "\n",
        "X = df.drop(columns=[\"opening_week_revenue\"])\n",
        "y = df[\"opening_week_revenue\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "UJtpfkUQuj8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_columns = X.columns\n"
      ],
      "metadata": {
        "id": "KjpKqgoc6cZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL EVALUATION\n",
        "Models are evaluated using R2, MAE, and RMSE to compare overall fit and prediction accuracy.\n",
        "\n",
        "R2 measures how well the model explains the variance in opening week revenue.\n",
        "MAE measures the average absolute prediction error.\n",
        "RMSE penalizes larger prediction errors more heavily.\n"
      ],
      "metadata": {
        "id": "w8Wl6Eqd64wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = train_and_evaluate_all_models(\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test\n",
        ")\n",
        "\n",
        "best_model_row = display_model_comparison(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SQ9FHH21gU3",
        "outputId": "506dd93e-85de-48e8-e5a6-7616ac0b7323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL COMPARISON (Ranked by R2)\n",
            "\n",
            "Rank   Model                R2         MAE             RMSE           \n",
            "-----------------------------------------------------------------\n",
            "1      LightGBM             0.6507     $11,881,880     $22,726,189    \n",
            "2      AdaBoost             0.6350     $14,267,580     $23,230,591    \n",
            "3      XGBoost              0.6280     $11,857,236     $23,452,367    \n",
            "4      Gradient Boosting    0.6108     $11,999,962     $23,988,197    \n",
            "5      Linear Regression    0.5887     $14,078,720     $24,659,901    \n",
            "6      Decision Tree        0.3305     $14,634,047     $31,462,747    \n",
            "7      SVR                  -0.1596    $22,544,496     $41,406,088    \n",
            "8      Neural Network       -0.3941    $24,141,041     $45,401,144    \n",
            "\n",
            "Best Model:\n",
            "Model: LightGBM\n",
            "R2: 0.6507\n",
            "MAE: $11,881,880\n",
            "RMSE: $22,726,189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AVATAR_MOVIE_NAME = \"Avatar: Fire and Ash\"\n",
        "AVATAR_RELEASE_YEAR = 2025\n",
        "OUTPUT_FILE = \"avatar_fire_and_ash_features.csv\""
      ],
      "metadata": {
        "id": "onsIc-JB3tLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Avatar: Fire and Ash DATA SCRAPING\n"
      ],
      "metadata": {
        "id": "J2fP3r7D6_2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_movie_on_tmdb(movie_name, year=None):\n",
        "    print(\"SEARCHING FOR AVATAR: FIRE AND ASH\")\n",
        "\n",
        "\n",
        "    url = \"https://api.themoviedb.org/3/search/movie\"\n",
        "    params = {\n",
        "        \"api_key\": TMDB_API_KEY,\n",
        "        \"query\": movie_name,\n",
        "        \"language\": \"en-US\"\n",
        "    }\n",
        "\n",
        "    if year:\n",
        "        params[\"year\"] = year\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    if not data.get(\"results\"):\n",
        "        print(\"✗ No results found on TMDB\")\n",
        "        return None\n",
        "\n",
        "    print(f\" Found {len(data['results'])} results\\n\")\n",
        "    for i, movie in enumerate(data[\"results\"][:5], 1):\n",
        "        print(f\"  {i}. {movie.get('title')} ({movie.get('release_date')})\")\n",
        "\n",
        "    chosen = data[\"results\"][0]\n",
        "    print(f\"\\n Using: {chosen.get('title')} (ID={chosen.get('id')})\\n\")\n",
        "\n",
        "    return chosen[\"id\"]\n",
        "\n",
        "\n",
        "def get_movie_details(movie_id):\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{movie_id}\"\n",
        "    params = {\n",
        "        \"api_key\": TMDB_API_KEY,\n",
        "        \"append_to_response\": \"credits,release_dates\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def extract_mpaa_rating(release_dates):\n",
        "    if not release_dates:\n",
        "        return \"NR\"\n",
        "\n",
        "    for country in release_dates.get(\"results\", []):\n",
        "        if country.get(\"iso_3166_1\") == \"US\":\n",
        "            for r in country.get(\"release_dates\", []):\n",
        "                if r.get(\"certification\"):\n",
        "                    return r[\"certification\"]\n",
        "\n",
        "    return \"NR\"\n",
        "\n",
        "\n",
        "\n",
        "def scrape_avatar_data(movie_name, year=None):\n",
        "    movie_id = search_movie_on_tmdb(movie_name, year)\n",
        "    if not movie_id:\n",
        "        return None\n",
        "\n",
        "    details = get_movie_details(movie_id)\n",
        "\n",
        "\n",
        "    genres = details.get(\"genres\", [])\n",
        "    genre_names = [g[\"name\"] for g in genres]\n",
        "\n",
        "    cast = details.get(\"credits\", {}).get(\"cast\", [])\n",
        "    crew = details.get(\"credits\", {}).get(\"crew\", [])\n",
        "\n",
        "    directors = [c[\"name\"] for c in crew if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    prod_companies = details.get(\"production_companies\", [])\n",
        "\n",
        "    avatar_data = {\n",
        "        \"title\": details.get(\"title\", \"\"),\n",
        "        \"release_date\": details.get(\"release_date\", \"\"),\n",
        "        \"tmdb_id\": details.get(\"id\"),\n",
        "        \"imdb_id\": details.get(\"imdb_id\", \"\"),\n",
        "        \"budget\": details.get(\"budget\", 0),\n",
        "        \"runtime\": details.get(\"runtime\", 0),\n",
        "        \"genres\": \"|\".join(genre_names),\n",
        "        \"country\": details.get(\"production_countries\", [{}])[0].get(\"iso_3166_1\", \"US\"),\n",
        "        \"mpaa_rating\": extract_mpaa_rating(details.get(\"release_dates\")),\n",
        "        \"popularity\": round(details.get(\"popularity\", 0), 2),\n",
        "        \"vote_average\": details.get(\"vote_average\", 0),\n",
        "        \"is_franchise\": 1 if details.get(\"belongs_to_collection\") else 0,\n",
        "        \"director\": directors[0] if directors else \"Unknown Director\",\n",
        "        \"actors\": \"|\".join([a[\"name\"] for a in cast[:5]]),\n",
        "        \"production_companies\": \"|\".join([p[\"name\"] for p in prod_companies]),\n",
        "        \"distributor_bom\": prod_companies[0][\"name\"] if prod_companies else \"Independent\"\n",
        "    }\n",
        "\n",
        "\n",
        "    print(\"AVATAR DATA EXTRACTED\")\n",
        "\n",
        "\n",
        "    for k, v in avatar_data.items():\n",
        "        if k not in [\"actors\", \"production_companies\", \"genres\"]:\n",
        "            print(f\"{k:25s}: {v}\")\n",
        "\n",
        "    return avatar_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "avatar_data = scrape_avatar_data(AVATAR_MOVIE_NAME, AVATAR_RELEASE_YEAR)\n",
        "\n",
        "if avatar_data:\n",
        "    df = pd.DataFrame([avatar_data])\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "\n",
        "    print(f\"Data saved to: {OUTPUT_FILE}\")\n",
        "    print(f\"Total features: {df.shape[1]}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\Failed to scrape Avatar data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-JR6qyQvKnj",
        "outputId": "2cd29a0a-b342-4e3a-b19d-0596d0a8da14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEARCHING FOR AVATAR: FIRE AND ASH\n",
            " Found 1 results\n",
            "\n",
            "  1. Avatar: Fire and Ash (2025-12-17)\n",
            "\n",
            " Using: Avatar: Fire and Ash (ID=83533)\n",
            "\n",
            "AVATAR DATA EXTRACTED\n",
            "title                    : Avatar: Fire and Ash\n",
            "release_date             : 2025-12-17\n",
            "tmdb_id                  : 83533\n",
            "imdb_id                  : tt1757678\n",
            "budget                   : 400000000\n",
            "runtime                  : 195\n",
            "country                  : US\n",
            "mpaa_rating              : PG-13\n",
            "popularity               : 121.0\n",
            "vote_average             : 0.0\n",
            "is_franchise             : 1\n",
            "director                 : James Cameron\n",
            "distributor_bom          : 20th Century Studios\n",
            "Data saved to: avatar_fire_and_ash_features.csv\n",
            "Total features: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL PREDICTION\n"
      ],
      "metadata": {
        "id": "Zu1bcR2H7iHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avatar_df = prepare_avatar_data(AVATAR_DATA, target_encodings)\n",
        "\n",
        "prediction = predict_avatar_with_best_model(\n",
        "    best_model_row,\n",
        "    scaler,\n",
        "    avatar_df,\n",
        "    training_columns\n",
        ")\n",
        "print(\"Avatar: Fire and Ash – Opening Week Prediction\")\n",
        "print(f\"Predicted opening week revenue: ${prediction:,.0f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jetpo7C1ckq",
        "outputId": "49fc7891-ed8a-4853-de52-ec280cf3eaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avatar: Fire and Ash – Opening Week Prediction\n",
            "Predicted opening week revenue: $183,641,693\n"
          ]
        }
      ]
    }
  ]
}